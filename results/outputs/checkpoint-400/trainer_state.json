{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.7494145199063232,
  "eval_steps": 500,
  "global_step": 400,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.001873536299765808,
      "grad_norm": 2.090939521789551,
      "learning_rate": 0.0,
      "loss": 1.7987,
      "step": 1
    },
    {
      "epoch": 0.003747072599531616,
      "grad_norm": 4.532320976257324,
      "learning_rate": 2e-05,
      "loss": 3.0404,
      "step": 2
    },
    {
      "epoch": 0.005620608899297424,
      "grad_norm": 2.762751579284668,
      "learning_rate": 4e-05,
      "loss": 2.3525,
      "step": 3
    },
    {
      "epoch": 0.007494145199063232,
      "grad_norm": 2.3622946739196777,
      "learning_rate": 6e-05,
      "loss": 1.8797,
      "step": 4
    },
    {
      "epoch": 0.00936768149882904,
      "grad_norm": 2.2987194061279297,
      "learning_rate": 8e-05,
      "loss": 2.2295,
      "step": 5
    },
    {
      "epoch": 0.011241217798594848,
      "grad_norm": 1.5390416383743286,
      "learning_rate": 0.0001,
      "loss": 1.6809,
      "step": 6
    },
    {
      "epoch": 0.013114754098360656,
      "grad_norm": 2.188498020172119,
      "learning_rate": 9.981096408317581e-05,
      "loss": 1.5392,
      "step": 7
    },
    {
      "epoch": 0.014988290398126464,
      "grad_norm": 1.551052212715149,
      "learning_rate": 9.962192816635161e-05,
      "loss": 1.6169,
      "step": 8
    },
    {
      "epoch": 0.01686182669789227,
      "grad_norm": 1.4383207559585571,
      "learning_rate": 9.943289224952742e-05,
      "loss": 1.3115,
      "step": 9
    },
    {
      "epoch": 0.01873536299765808,
      "grad_norm": 2.7969095706939697,
      "learning_rate": 9.924385633270322e-05,
      "loss": 1.4285,
      "step": 10
    },
    {
      "epoch": 0.020608899297423888,
      "grad_norm": 4.1018757820129395,
      "learning_rate": 9.905482041587902e-05,
      "loss": 1.3653,
      "step": 11
    },
    {
      "epoch": 0.022482435597189696,
      "grad_norm": 2.223867416381836,
      "learning_rate": 9.886578449905483e-05,
      "loss": 1.3271,
      "step": 12
    },
    {
      "epoch": 0.024355971896955503,
      "grad_norm": 7.061211109161377,
      "learning_rate": 9.867674858223063e-05,
      "loss": 2.01,
      "step": 13
    },
    {
      "epoch": 0.02622950819672131,
      "grad_norm": 1.955973505973816,
      "learning_rate": 9.848771266540643e-05,
      "loss": 1.2968,
      "step": 14
    },
    {
      "epoch": 0.02810304449648712,
      "grad_norm": 1.4552587270736694,
      "learning_rate": 9.829867674858224e-05,
      "loss": 0.8981,
      "step": 15
    },
    {
      "epoch": 0.029976580796252927,
      "grad_norm": 2.794006824493408,
      "learning_rate": 9.810964083175804e-05,
      "loss": 1.3493,
      "step": 16
    },
    {
      "epoch": 0.03185011709601874,
      "grad_norm": 1.4143027067184448,
      "learning_rate": 9.792060491493384e-05,
      "loss": 0.6672,
      "step": 17
    },
    {
      "epoch": 0.03372365339578454,
      "grad_norm": 1.8299640417099,
      "learning_rate": 9.773156899810965e-05,
      "loss": 1.1026,
      "step": 18
    },
    {
      "epoch": 0.035597189695550355,
      "grad_norm": 2.621678352355957,
      "learning_rate": 9.754253308128545e-05,
      "loss": 1.166,
      "step": 19
    },
    {
      "epoch": 0.03747072599531616,
      "grad_norm": 1.939223289489746,
      "learning_rate": 9.735349716446125e-05,
      "loss": 1.0929,
      "step": 20
    },
    {
      "epoch": 0.03934426229508197,
      "grad_norm": 1.6975116729736328,
      "learning_rate": 9.716446124763706e-05,
      "loss": 0.8331,
      "step": 21
    },
    {
      "epoch": 0.041217798594847775,
      "grad_norm": 1.4748836755752563,
      "learning_rate": 9.697542533081286e-05,
      "loss": 1.068,
      "step": 22
    },
    {
      "epoch": 0.04309133489461359,
      "grad_norm": 1.288405179977417,
      "learning_rate": 9.678638941398866e-05,
      "loss": 0.8119,
      "step": 23
    },
    {
      "epoch": 0.04496487119437939,
      "grad_norm": 0.9960367679595947,
      "learning_rate": 9.659735349716447e-05,
      "loss": 0.7201,
      "step": 24
    },
    {
      "epoch": 0.0468384074941452,
      "grad_norm": 1.749763011932373,
      "learning_rate": 9.640831758034027e-05,
      "loss": 1.0778,
      "step": 25
    },
    {
      "epoch": 0.04871194379391101,
      "grad_norm": 2.176844358444214,
      "learning_rate": 9.621928166351607e-05,
      "loss": 0.8797,
      "step": 26
    },
    {
      "epoch": 0.05058548009367682,
      "grad_norm": 1.5331953763961792,
      "learning_rate": 9.603024574669188e-05,
      "loss": 0.8036,
      "step": 27
    },
    {
      "epoch": 0.05245901639344262,
      "grad_norm": 1.9499155282974243,
      "learning_rate": 9.584120982986768e-05,
      "loss": 0.792,
      "step": 28
    },
    {
      "epoch": 0.054332552693208434,
      "grad_norm": 1.1683961153030396,
      "learning_rate": 9.565217391304348e-05,
      "loss": 0.6993,
      "step": 29
    },
    {
      "epoch": 0.05620608899297424,
      "grad_norm": 1.1325722932815552,
      "learning_rate": 9.546313799621929e-05,
      "loss": 0.816,
      "step": 30
    },
    {
      "epoch": 0.05807962529274005,
      "grad_norm": 1.6469318866729736,
      "learning_rate": 9.527410207939509e-05,
      "loss": 0.8478,
      "step": 31
    },
    {
      "epoch": 0.059953161592505855,
      "grad_norm": 1.130900263786316,
      "learning_rate": 9.50850661625709e-05,
      "loss": 0.8302,
      "step": 32
    },
    {
      "epoch": 0.061826697892271666,
      "grad_norm": 1.6520392894744873,
      "learning_rate": 9.48960302457467e-05,
      "loss": 0.8494,
      "step": 33
    },
    {
      "epoch": 0.06370023419203748,
      "grad_norm": 1.3386486768722534,
      "learning_rate": 9.47069943289225e-05,
      "loss": 1.1646,
      "step": 34
    },
    {
      "epoch": 0.06557377049180328,
      "grad_norm": 1.724180817604065,
      "learning_rate": 9.45179584120983e-05,
      "loss": 0.8506,
      "step": 35
    },
    {
      "epoch": 0.06744730679156909,
      "grad_norm": 1.1362251043319702,
      "learning_rate": 9.432892249527411e-05,
      "loss": 0.766,
      "step": 36
    },
    {
      "epoch": 0.06932084309133489,
      "grad_norm": 2.341891050338745,
      "learning_rate": 9.413988657844991e-05,
      "loss": 0.9629,
      "step": 37
    },
    {
      "epoch": 0.07119437939110071,
      "grad_norm": 2.1718108654022217,
      "learning_rate": 9.395085066162571e-05,
      "loss": 1.0492,
      "step": 38
    },
    {
      "epoch": 0.07306791569086651,
      "grad_norm": 1.355704426765442,
      "learning_rate": 9.376181474480152e-05,
      "loss": 0.7644,
      "step": 39
    },
    {
      "epoch": 0.07494145199063232,
      "grad_norm": 1.651464581489563,
      "learning_rate": 9.357277882797732e-05,
      "loss": 1.011,
      "step": 40
    },
    {
      "epoch": 0.07681498829039812,
      "grad_norm": 0.8621445894241333,
      "learning_rate": 9.338374291115312e-05,
      "loss": 0.6145,
      "step": 41
    },
    {
      "epoch": 0.07868852459016394,
      "grad_norm": 1.4339054822921753,
      "learning_rate": 9.319470699432893e-05,
      "loss": 0.922,
      "step": 42
    },
    {
      "epoch": 0.08056206088992975,
      "grad_norm": 0.9171818494796753,
      "learning_rate": 9.300567107750473e-05,
      "loss": 0.7063,
      "step": 43
    },
    {
      "epoch": 0.08243559718969555,
      "grad_norm": 1.710557460784912,
      "learning_rate": 9.281663516068053e-05,
      "loss": 0.7612,
      "step": 44
    },
    {
      "epoch": 0.08430913348946135,
      "grad_norm": 0.7321911454200745,
      "learning_rate": 9.262759924385634e-05,
      "loss": 0.5966,
      "step": 45
    },
    {
      "epoch": 0.08618266978922717,
      "grad_norm": 1.2243731021881104,
      "learning_rate": 9.243856332703214e-05,
      "loss": 0.7608,
      "step": 46
    },
    {
      "epoch": 0.08805620608899298,
      "grad_norm": 1.4489336013793945,
      "learning_rate": 9.224952741020794e-05,
      "loss": 0.6977,
      "step": 47
    },
    {
      "epoch": 0.08992974238875878,
      "grad_norm": 0.8822866678237915,
      "learning_rate": 9.206049149338375e-05,
      "loss": 0.5098,
      "step": 48
    },
    {
      "epoch": 0.09180327868852459,
      "grad_norm": 1.6500988006591797,
      "learning_rate": 9.187145557655955e-05,
      "loss": 0.9276,
      "step": 49
    },
    {
      "epoch": 0.0936768149882904,
      "grad_norm": 1.362795352935791,
      "learning_rate": 9.168241965973536e-05,
      "loss": 0.8429,
      "step": 50
    },
    {
      "epoch": 0.09555035128805621,
      "grad_norm": 1.1094541549682617,
      "learning_rate": 9.149338374291116e-05,
      "loss": 0.7539,
      "step": 51
    },
    {
      "epoch": 0.09742388758782201,
      "grad_norm": 1.6482110023498535,
      "learning_rate": 9.130434782608696e-05,
      "loss": 0.8098,
      "step": 52
    },
    {
      "epoch": 0.09929742388758782,
      "grad_norm": 1.2354305982589722,
      "learning_rate": 9.111531190926277e-05,
      "loss": 0.7911,
      "step": 53
    },
    {
      "epoch": 0.10117096018735364,
      "grad_norm": 1.3218321800231934,
      "learning_rate": 9.092627599243857e-05,
      "loss": 0.9112,
      "step": 54
    },
    {
      "epoch": 0.10304449648711944,
      "grad_norm": 1.3674978017807007,
      "learning_rate": 9.073724007561437e-05,
      "loss": 0.8139,
      "step": 55
    },
    {
      "epoch": 0.10491803278688525,
      "grad_norm": 1.0735994577407837,
      "learning_rate": 9.054820415879018e-05,
      "loss": 0.5909,
      "step": 56
    },
    {
      "epoch": 0.10679156908665105,
      "grad_norm": 1.2946735620498657,
      "learning_rate": 9.035916824196598e-05,
      "loss": 0.6146,
      "step": 57
    },
    {
      "epoch": 0.10866510538641687,
      "grad_norm": 0.9370225667953491,
      "learning_rate": 9.017013232514178e-05,
      "loss": 0.6058,
      "step": 58
    },
    {
      "epoch": 0.11053864168618267,
      "grad_norm": 1.5899443626403809,
      "learning_rate": 8.998109640831759e-05,
      "loss": 0.9635,
      "step": 59
    },
    {
      "epoch": 0.11241217798594848,
      "grad_norm": 1.1521223783493042,
      "learning_rate": 8.979206049149339e-05,
      "loss": 0.68,
      "step": 60
    },
    {
      "epoch": 0.11428571428571428,
      "grad_norm": 1.056638479232788,
      "learning_rate": 8.960302457466919e-05,
      "loss": 0.5551,
      "step": 61
    },
    {
      "epoch": 0.1161592505854801,
      "grad_norm": 1.0678298473358154,
      "learning_rate": 8.9413988657845e-05,
      "loss": 0.7621,
      "step": 62
    },
    {
      "epoch": 0.1180327868852459,
      "grad_norm": 1.539071798324585,
      "learning_rate": 8.92249527410208e-05,
      "loss": 0.7727,
      "step": 63
    },
    {
      "epoch": 0.11990632318501171,
      "grad_norm": 1.6142797470092773,
      "learning_rate": 8.90359168241966e-05,
      "loss": 0.9425,
      "step": 64
    },
    {
      "epoch": 0.12177985948477751,
      "grad_norm": 1.7965556383132935,
      "learning_rate": 8.88468809073724e-05,
      "loss": 0.5933,
      "step": 65
    },
    {
      "epoch": 0.12365339578454333,
      "grad_norm": 1.67196786403656,
      "learning_rate": 8.865784499054821e-05,
      "loss": 0.8487,
      "step": 66
    },
    {
      "epoch": 0.12552693208430912,
      "grad_norm": 1.8122950792312622,
      "learning_rate": 8.846880907372401e-05,
      "loss": 0.8707,
      "step": 67
    },
    {
      "epoch": 0.12740046838407496,
      "grad_norm": 1.2698214054107666,
      "learning_rate": 8.827977315689982e-05,
      "loss": 0.8352,
      "step": 68
    },
    {
      "epoch": 0.12927400468384076,
      "grad_norm": 0.8460606336593628,
      "learning_rate": 8.809073724007562e-05,
      "loss": 0.4941,
      "step": 69
    },
    {
      "epoch": 0.13114754098360656,
      "grad_norm": 1.7003281116485596,
      "learning_rate": 8.790170132325142e-05,
      "loss": 0.5584,
      "step": 70
    },
    {
      "epoch": 0.13302107728337237,
      "grad_norm": 0.9534781575202942,
      "learning_rate": 8.771266540642723e-05,
      "loss": 0.4676,
      "step": 71
    },
    {
      "epoch": 0.13489461358313817,
      "grad_norm": 1.7375677824020386,
      "learning_rate": 8.752362948960303e-05,
      "loss": 0.866,
      "step": 72
    },
    {
      "epoch": 0.13676814988290398,
      "grad_norm": 1.2173718214035034,
      "learning_rate": 8.733459357277883e-05,
      "loss": 0.5583,
      "step": 73
    },
    {
      "epoch": 0.13864168618266978,
      "grad_norm": 1.384255290031433,
      "learning_rate": 8.714555765595464e-05,
      "loss": 0.7437,
      "step": 74
    },
    {
      "epoch": 0.1405152224824356,
      "grad_norm": 1.9493659734725952,
      "learning_rate": 8.695652173913044e-05,
      "loss": 0.6496,
      "step": 75
    },
    {
      "epoch": 0.14238875878220142,
      "grad_norm": 1.1643542051315308,
      "learning_rate": 8.676748582230624e-05,
      "loss": 0.5214,
      "step": 76
    },
    {
      "epoch": 0.14426229508196722,
      "grad_norm": 1.6060214042663574,
      "learning_rate": 8.657844990548205e-05,
      "loss": 0.9404,
      "step": 77
    },
    {
      "epoch": 0.14613583138173303,
      "grad_norm": 1.1772866249084473,
      "learning_rate": 8.638941398865785e-05,
      "loss": 0.6202,
      "step": 78
    },
    {
      "epoch": 0.14800936768149883,
      "grad_norm": 1.7058088779449463,
      "learning_rate": 8.620037807183365e-05,
      "loss": 0.7873,
      "step": 79
    },
    {
      "epoch": 0.14988290398126464,
      "grad_norm": 2.16811466217041,
      "learning_rate": 8.601134215500946e-05,
      "loss": 0.6352,
      "step": 80
    },
    {
      "epoch": 0.15175644028103044,
      "grad_norm": 1.3399467468261719,
      "learning_rate": 8.582230623818526e-05,
      "loss": 0.9051,
      "step": 81
    },
    {
      "epoch": 0.15362997658079625,
      "grad_norm": 1.1082653999328613,
      "learning_rate": 8.563327032136106e-05,
      "loss": 0.8084,
      "step": 82
    },
    {
      "epoch": 0.15550351288056205,
      "grad_norm": 1.3119242191314697,
      "learning_rate": 8.544423440453687e-05,
      "loss": 0.7559,
      "step": 83
    },
    {
      "epoch": 0.15737704918032788,
      "grad_norm": 2.3345730304718018,
      "learning_rate": 8.525519848771267e-05,
      "loss": 0.7894,
      "step": 84
    },
    {
      "epoch": 0.1592505854800937,
      "grad_norm": 2.0438265800476074,
      "learning_rate": 8.506616257088847e-05,
      "loss": 0.6699,
      "step": 85
    },
    {
      "epoch": 0.1611241217798595,
      "grad_norm": 1.3512864112854004,
      "learning_rate": 8.487712665406428e-05,
      "loss": 0.8187,
      "step": 86
    },
    {
      "epoch": 0.1629976580796253,
      "grad_norm": 1.5489726066589355,
      "learning_rate": 8.468809073724008e-05,
      "loss": 0.7688,
      "step": 87
    },
    {
      "epoch": 0.1648711943793911,
      "grad_norm": 1.4700185060501099,
      "learning_rate": 8.449905482041589e-05,
      "loss": 0.5736,
      "step": 88
    },
    {
      "epoch": 0.1667447306791569,
      "grad_norm": 2.1427993774414062,
      "learning_rate": 8.431001890359168e-05,
      "loss": 0.6863,
      "step": 89
    },
    {
      "epoch": 0.1686182669789227,
      "grad_norm": 1.278503656387329,
      "learning_rate": 8.412098298676749e-05,
      "loss": 0.6965,
      "step": 90
    },
    {
      "epoch": 0.17049180327868851,
      "grad_norm": 1.2657763957977295,
      "learning_rate": 8.39319470699433e-05,
      "loss": 0.8239,
      "step": 91
    },
    {
      "epoch": 0.17236533957845435,
      "grad_norm": 1.2226310968399048,
      "learning_rate": 8.37429111531191e-05,
      "loss": 0.5969,
      "step": 92
    },
    {
      "epoch": 0.17423887587822015,
      "grad_norm": 2.2177038192749023,
      "learning_rate": 8.35538752362949e-05,
      "loss": 0.7925,
      "step": 93
    },
    {
      "epoch": 0.17611241217798596,
      "grad_norm": 2.1600499153137207,
      "learning_rate": 8.33648393194707e-05,
      "loss": 0.7191,
      "step": 94
    },
    {
      "epoch": 0.17798594847775176,
      "grad_norm": 1.1715075969696045,
      "learning_rate": 8.31758034026465e-05,
      "loss": 0.5875,
      "step": 95
    },
    {
      "epoch": 0.17985948477751756,
      "grad_norm": 2.0651919841766357,
      "learning_rate": 8.298676748582231e-05,
      "loss": 0.8025,
      "step": 96
    },
    {
      "epoch": 0.18173302107728337,
      "grad_norm": 0.9527477622032166,
      "learning_rate": 8.279773156899812e-05,
      "loss": 0.4588,
      "step": 97
    },
    {
      "epoch": 0.18360655737704917,
      "grad_norm": 1.5065048933029175,
      "learning_rate": 8.260869565217392e-05,
      "loss": 0.8781,
      "step": 98
    },
    {
      "epoch": 0.18548009367681498,
      "grad_norm": 1.473615288734436,
      "learning_rate": 8.241965973534972e-05,
      "loss": 0.9785,
      "step": 99
    },
    {
      "epoch": 0.1873536299765808,
      "grad_norm": 1.1820892095565796,
      "learning_rate": 8.223062381852553e-05,
      "loss": 0.4682,
      "step": 100
    },
    {
      "epoch": 0.18922716627634661,
      "grad_norm": 0.8920969367027283,
      "learning_rate": 8.204158790170132e-05,
      "loss": 0.4945,
      "step": 101
    },
    {
      "epoch": 0.19110070257611242,
      "grad_norm": 1.2156152725219727,
      "learning_rate": 8.185255198487713e-05,
      "loss": 0.4777,
      "step": 102
    },
    {
      "epoch": 0.19297423887587822,
      "grad_norm": 1.8212642669677734,
      "learning_rate": 8.166351606805294e-05,
      "loss": 0.5483,
      "step": 103
    },
    {
      "epoch": 0.19484777517564403,
      "grad_norm": 1.1625745296478271,
      "learning_rate": 8.147448015122874e-05,
      "loss": 0.643,
      "step": 104
    },
    {
      "epoch": 0.19672131147540983,
      "grad_norm": 1.9231617450714111,
      "learning_rate": 8.128544423440454e-05,
      "loss": 0.5132,
      "step": 105
    },
    {
      "epoch": 0.19859484777517564,
      "grad_norm": 1.316473126411438,
      "learning_rate": 8.109640831758035e-05,
      "loss": 0.773,
      "step": 106
    },
    {
      "epoch": 0.20046838407494144,
      "grad_norm": 1.8105800151824951,
      "learning_rate": 8.090737240075614e-05,
      "loss": 0.9024,
      "step": 107
    },
    {
      "epoch": 0.20234192037470727,
      "grad_norm": 1.188331127166748,
      "learning_rate": 8.071833648393195e-05,
      "loss": 0.603,
      "step": 108
    },
    {
      "epoch": 0.20421545667447308,
      "grad_norm": 1.0518218278884888,
      "learning_rate": 8.052930056710776e-05,
      "loss": 0.6667,
      "step": 109
    },
    {
      "epoch": 0.20608899297423888,
      "grad_norm": 1.6687662601470947,
      "learning_rate": 8.034026465028356e-05,
      "loss": 0.9896,
      "step": 110
    },
    {
      "epoch": 0.2079625292740047,
      "grad_norm": 1.658023715019226,
      "learning_rate": 8.015122873345936e-05,
      "loss": 0.5899,
      "step": 111
    },
    {
      "epoch": 0.2098360655737705,
      "grad_norm": 1.3719006776809692,
      "learning_rate": 7.996219281663517e-05,
      "loss": 0.7587,
      "step": 112
    },
    {
      "epoch": 0.2117096018735363,
      "grad_norm": 2.2038848400115967,
      "learning_rate": 7.977315689981096e-05,
      "loss": 1.0373,
      "step": 113
    },
    {
      "epoch": 0.2135831381733021,
      "grad_norm": 1.21440851688385,
      "learning_rate": 7.958412098298677e-05,
      "loss": 0.595,
      "step": 114
    },
    {
      "epoch": 0.2154566744730679,
      "grad_norm": 1.162292718887329,
      "learning_rate": 7.939508506616258e-05,
      "loss": 0.5667,
      "step": 115
    },
    {
      "epoch": 0.21733021077283374,
      "grad_norm": 1.6409554481506348,
      "learning_rate": 7.920604914933838e-05,
      "loss": 0.9673,
      "step": 116
    },
    {
      "epoch": 0.21920374707259954,
      "grad_norm": 1.2641971111297607,
      "learning_rate": 7.901701323251418e-05,
      "loss": 0.55,
      "step": 117
    },
    {
      "epoch": 0.22107728337236535,
      "grad_norm": 1.61600923538208,
      "learning_rate": 7.882797731568999e-05,
      "loss": 0.6676,
      "step": 118
    },
    {
      "epoch": 0.22295081967213115,
      "grad_norm": 1.1525450944900513,
      "learning_rate": 7.863894139886578e-05,
      "loss": 0.5948,
      "step": 119
    },
    {
      "epoch": 0.22482435597189696,
      "grad_norm": 1.0737229585647583,
      "learning_rate": 7.84499054820416e-05,
      "loss": 0.6325,
      "step": 120
    },
    {
      "epoch": 0.22669789227166276,
      "grad_norm": 2.15238094329834,
      "learning_rate": 7.82608695652174e-05,
      "loss": 0.8463,
      "step": 121
    },
    {
      "epoch": 0.22857142857142856,
      "grad_norm": 1.4500391483306885,
      "learning_rate": 7.80718336483932e-05,
      "loss": 0.6874,
      "step": 122
    },
    {
      "epoch": 0.23044496487119437,
      "grad_norm": 1.7167143821716309,
      "learning_rate": 7.7882797731569e-05,
      "loss": 0.3973,
      "step": 123
    },
    {
      "epoch": 0.2323185011709602,
      "grad_norm": 1.349351167678833,
      "learning_rate": 7.769376181474481e-05,
      "loss": 0.6138,
      "step": 124
    },
    {
      "epoch": 0.234192037470726,
      "grad_norm": 2.234450101852417,
      "learning_rate": 7.75047258979206e-05,
      "loss": 0.8304,
      "step": 125
    },
    {
      "epoch": 0.2360655737704918,
      "grad_norm": 2.1716301441192627,
      "learning_rate": 7.731568998109642e-05,
      "loss": 0.6858,
      "step": 126
    },
    {
      "epoch": 0.23793911007025761,
      "grad_norm": 0.8116918802261353,
      "learning_rate": 7.712665406427222e-05,
      "loss": 0.5914,
      "step": 127
    },
    {
      "epoch": 0.23981264637002342,
      "grad_norm": 1.1821343898773193,
      "learning_rate": 7.693761814744802e-05,
      "loss": 0.6736,
      "step": 128
    },
    {
      "epoch": 0.24168618266978922,
      "grad_norm": 1.0376981496810913,
      "learning_rate": 7.674858223062383e-05,
      "loss": 0.7789,
      "step": 129
    },
    {
      "epoch": 0.24355971896955503,
      "grad_norm": 1.9261983633041382,
      "learning_rate": 7.655954631379963e-05,
      "loss": 0.9256,
      "step": 130
    },
    {
      "epoch": 0.24543325526932083,
      "grad_norm": 1.31080961227417,
      "learning_rate": 7.637051039697543e-05,
      "loss": 0.5582,
      "step": 131
    },
    {
      "epoch": 0.24730679156908666,
      "grad_norm": 1.3912689685821533,
      "learning_rate": 7.618147448015122e-05,
      "loss": 0.516,
      "step": 132
    },
    {
      "epoch": 0.24918032786885247,
      "grad_norm": 0.9548578262329102,
      "learning_rate": 7.599243856332704e-05,
      "loss": 0.4305,
      "step": 133
    },
    {
      "epoch": 0.25105386416861825,
      "grad_norm": 1.14516282081604,
      "learning_rate": 7.580340264650284e-05,
      "loss": 0.66,
      "step": 134
    },
    {
      "epoch": 0.2529274004683841,
      "grad_norm": 2.237309455871582,
      "learning_rate": 7.561436672967865e-05,
      "loss": 0.9452,
      "step": 135
    },
    {
      "epoch": 0.2548009367681499,
      "grad_norm": 1.4984512329101562,
      "learning_rate": 7.542533081285445e-05,
      "loss": 0.5785,
      "step": 136
    },
    {
      "epoch": 0.2566744730679157,
      "grad_norm": 2.009812831878662,
      "learning_rate": 7.523629489603025e-05,
      "loss": 0.8133,
      "step": 137
    },
    {
      "epoch": 0.2585480093676815,
      "grad_norm": 1.9449831247329712,
      "learning_rate": 7.504725897920604e-05,
      "loss": 0.8105,
      "step": 138
    },
    {
      "epoch": 0.2604215456674473,
      "grad_norm": 1.8404974937438965,
      "learning_rate": 7.485822306238186e-05,
      "loss": 0.8549,
      "step": 139
    },
    {
      "epoch": 0.26229508196721313,
      "grad_norm": 1.520268201828003,
      "learning_rate": 7.466918714555766e-05,
      "loss": 0.7983,
      "step": 140
    },
    {
      "epoch": 0.2641686182669789,
      "grad_norm": 1.5690233707427979,
      "learning_rate": 7.448015122873347e-05,
      "loss": 0.4223,
      "step": 141
    },
    {
      "epoch": 0.26604215456674474,
      "grad_norm": 2.602778911590576,
      "learning_rate": 7.429111531190927e-05,
      "loss": 0.9803,
      "step": 142
    },
    {
      "epoch": 0.2679156908665105,
      "grad_norm": 0.7089161276817322,
      "learning_rate": 7.410207939508507e-05,
      "loss": 0.4447,
      "step": 143
    },
    {
      "epoch": 0.26978922716627635,
      "grad_norm": 2.723905086517334,
      "learning_rate": 7.391304347826086e-05,
      "loss": 0.944,
      "step": 144
    },
    {
      "epoch": 0.2716627634660422,
      "grad_norm": 1.5165345668792725,
      "learning_rate": 7.372400756143668e-05,
      "loss": 0.5407,
      "step": 145
    },
    {
      "epoch": 0.27353629976580796,
      "grad_norm": 1.630317211151123,
      "learning_rate": 7.353497164461248e-05,
      "loss": 0.7407,
      "step": 146
    },
    {
      "epoch": 0.2754098360655738,
      "grad_norm": 1.837120771408081,
      "learning_rate": 7.334593572778829e-05,
      "loss": 0.5481,
      "step": 147
    },
    {
      "epoch": 0.27728337236533956,
      "grad_norm": 1.2072473764419556,
      "learning_rate": 7.315689981096409e-05,
      "loss": 0.5156,
      "step": 148
    },
    {
      "epoch": 0.2791569086651054,
      "grad_norm": 2.421882390975952,
      "learning_rate": 7.29678638941399e-05,
      "loss": 0.7765,
      "step": 149
    },
    {
      "epoch": 0.2810304449648712,
      "grad_norm": 1.3334547281265259,
      "learning_rate": 7.277882797731568e-05,
      "loss": 0.5838,
      "step": 150
    },
    {
      "epoch": 0.282903981264637,
      "grad_norm": 2.579413414001465,
      "learning_rate": 7.25897920604915e-05,
      "loss": 0.9001,
      "step": 151
    },
    {
      "epoch": 0.28477751756440284,
      "grad_norm": 1.3832082748413086,
      "learning_rate": 7.24007561436673e-05,
      "loss": 0.3774,
      "step": 152
    },
    {
      "epoch": 0.2866510538641686,
      "grad_norm": 2.0023980140686035,
      "learning_rate": 7.221172022684311e-05,
      "loss": 0.8282,
      "step": 153
    },
    {
      "epoch": 0.28852459016393445,
      "grad_norm": 1.2935445308685303,
      "learning_rate": 7.202268431001891e-05,
      "loss": 0.5882,
      "step": 154
    },
    {
      "epoch": 0.2903981264637002,
      "grad_norm": 1.066869854927063,
      "learning_rate": 7.183364839319471e-05,
      "loss": 0.4801,
      "step": 155
    },
    {
      "epoch": 0.29227166276346606,
      "grad_norm": 1.6649844646453857,
      "learning_rate": 7.16446124763705e-05,
      "loss": 0.8323,
      "step": 156
    },
    {
      "epoch": 0.29414519906323183,
      "grad_norm": 1.3529393672943115,
      "learning_rate": 7.145557655954632e-05,
      "loss": 0.9201,
      "step": 157
    },
    {
      "epoch": 0.29601873536299766,
      "grad_norm": 1.8929071426391602,
      "learning_rate": 7.126654064272212e-05,
      "loss": 0.8485,
      "step": 158
    },
    {
      "epoch": 0.29789227166276344,
      "grad_norm": 1.4839035272598267,
      "learning_rate": 7.107750472589793e-05,
      "loss": 0.8995,
      "step": 159
    },
    {
      "epoch": 0.2997658079625293,
      "grad_norm": 1.4978033304214478,
      "learning_rate": 7.088846880907373e-05,
      "loss": 0.6125,
      "step": 160
    },
    {
      "epoch": 0.3016393442622951,
      "grad_norm": 2.7162442207336426,
      "learning_rate": 7.069943289224953e-05,
      "loss": 0.7321,
      "step": 161
    },
    {
      "epoch": 0.3035128805620609,
      "grad_norm": 1.2217638492584229,
      "learning_rate": 7.051039697542532e-05,
      "loss": 0.6523,
      "step": 162
    },
    {
      "epoch": 0.3053864168618267,
      "grad_norm": 1.8327504396438599,
      "learning_rate": 7.032136105860114e-05,
      "loss": 0.4452,
      "step": 163
    },
    {
      "epoch": 0.3072599531615925,
      "grad_norm": 1.6773335933685303,
      "learning_rate": 7.013232514177695e-05,
      "loss": 0.7429,
      "step": 164
    },
    {
      "epoch": 0.3091334894613583,
      "grad_norm": 1.8174676895141602,
      "learning_rate": 6.994328922495275e-05,
      "loss": 0.8308,
      "step": 165
    },
    {
      "epoch": 0.3110070257611241,
      "grad_norm": 2.2867977619171143,
      "learning_rate": 6.975425330812855e-05,
      "loss": 0.7868,
      "step": 166
    },
    {
      "epoch": 0.31288056206088993,
      "grad_norm": 1.6591026782989502,
      "learning_rate": 6.956521739130436e-05,
      "loss": 0.6603,
      "step": 167
    },
    {
      "epoch": 0.31475409836065577,
      "grad_norm": 1.8519792556762695,
      "learning_rate": 6.937618147448015e-05,
      "loss": 0.6835,
      "step": 168
    },
    {
      "epoch": 0.31662763466042154,
      "grad_norm": 1.3662583827972412,
      "learning_rate": 6.918714555765595e-05,
      "loss": 0.6254,
      "step": 169
    },
    {
      "epoch": 0.3185011709601874,
      "grad_norm": 1.6338391304016113,
      "learning_rate": 6.899810964083177e-05,
      "loss": 0.6292,
      "step": 170
    },
    {
      "epoch": 0.32037470725995315,
      "grad_norm": 0.7478632926940918,
      "learning_rate": 6.880907372400757e-05,
      "loss": 0.4818,
      "step": 171
    },
    {
      "epoch": 0.322248243559719,
      "grad_norm": 0.9037790894508362,
      "learning_rate": 6.862003780718337e-05,
      "loss": 0.5059,
      "step": 172
    },
    {
      "epoch": 0.32412177985948476,
      "grad_norm": 2.0174107551574707,
      "learning_rate": 6.843100189035918e-05,
      "loss": 0.7364,
      "step": 173
    },
    {
      "epoch": 0.3259953161592506,
      "grad_norm": 1.4936574697494507,
      "learning_rate": 6.824196597353497e-05,
      "loss": 0.6877,
      "step": 174
    },
    {
      "epoch": 0.32786885245901637,
      "grad_norm": 1.2900583744049072,
      "learning_rate": 6.805293005671077e-05,
      "loss": 0.655,
      "step": 175
    },
    {
      "epoch": 0.3297423887587822,
      "grad_norm": 1.4614814519882202,
      "learning_rate": 6.786389413988659e-05,
      "loss": 0.4767,
      "step": 176
    },
    {
      "epoch": 0.33161592505854803,
      "grad_norm": 1.8233654499053955,
      "learning_rate": 6.767485822306239e-05,
      "loss": 0.6718,
      "step": 177
    },
    {
      "epoch": 0.3334894613583138,
      "grad_norm": 1.6076382398605347,
      "learning_rate": 6.748582230623819e-05,
      "loss": 0.5725,
      "step": 178
    },
    {
      "epoch": 0.33536299765807964,
      "grad_norm": 1.6340928077697754,
      "learning_rate": 6.7296786389414e-05,
      "loss": 0.7243,
      "step": 179
    },
    {
      "epoch": 0.3372365339578454,
      "grad_norm": 1.2838597297668457,
      "learning_rate": 6.710775047258979e-05,
      "loss": 0.7768,
      "step": 180
    },
    {
      "epoch": 0.33911007025761125,
      "grad_norm": 2.3067755699157715,
      "learning_rate": 6.691871455576559e-05,
      "loss": 0.7435,
      "step": 181
    },
    {
      "epoch": 0.34098360655737703,
      "grad_norm": 1.7460582256317139,
      "learning_rate": 6.67296786389414e-05,
      "loss": 0.4459,
      "step": 182
    },
    {
      "epoch": 0.34285714285714286,
      "grad_norm": 1.515345811843872,
      "learning_rate": 6.654064272211721e-05,
      "loss": 0.6699,
      "step": 183
    },
    {
      "epoch": 0.3447306791569087,
      "grad_norm": 2.143749237060547,
      "learning_rate": 6.635160680529301e-05,
      "loss": 0.6025,
      "step": 184
    },
    {
      "epoch": 0.34660421545667447,
      "grad_norm": 1.5271471738815308,
      "learning_rate": 6.616257088846882e-05,
      "loss": 0.6224,
      "step": 185
    },
    {
      "epoch": 0.3484777517564403,
      "grad_norm": 2.496366262435913,
      "learning_rate": 6.59735349716446e-05,
      "loss": 0.86,
      "step": 186
    },
    {
      "epoch": 0.3503512880562061,
      "grad_norm": 1.5596503019332886,
      "learning_rate": 6.578449905482041e-05,
      "loss": 0.4789,
      "step": 187
    },
    {
      "epoch": 0.3522248243559719,
      "grad_norm": 1.47548246383667,
      "learning_rate": 6.559546313799623e-05,
      "loss": 0.5994,
      "step": 188
    },
    {
      "epoch": 0.3540983606557377,
      "grad_norm": 1.4049471616744995,
      "learning_rate": 6.540642722117203e-05,
      "loss": 0.5472,
      "step": 189
    },
    {
      "epoch": 0.3559718969555035,
      "grad_norm": 1.998459815979004,
      "learning_rate": 6.521739130434783e-05,
      "loss": 0.7429,
      "step": 190
    },
    {
      "epoch": 0.3578454332552693,
      "grad_norm": 1.6132440567016602,
      "learning_rate": 6.502835538752364e-05,
      "loss": 0.6108,
      "step": 191
    },
    {
      "epoch": 0.35971896955503513,
      "grad_norm": 1.468039870262146,
      "learning_rate": 6.483931947069943e-05,
      "loss": 0.6584,
      "step": 192
    },
    {
      "epoch": 0.36159250585480096,
      "grad_norm": 1.3125429153442383,
      "learning_rate": 6.465028355387523e-05,
      "loss": 0.5668,
      "step": 193
    },
    {
      "epoch": 0.36346604215456674,
      "grad_norm": 1.4213072061538696,
      "learning_rate": 6.446124763705105e-05,
      "loss": 0.5671,
      "step": 194
    },
    {
      "epoch": 0.36533957845433257,
      "grad_norm": 1.265404224395752,
      "learning_rate": 6.427221172022685e-05,
      "loss": 0.6532,
      "step": 195
    },
    {
      "epoch": 0.36721311475409835,
      "grad_norm": 2.4433231353759766,
      "learning_rate": 6.408317580340265e-05,
      "loss": 0.7053,
      "step": 196
    },
    {
      "epoch": 0.3690866510538642,
      "grad_norm": 2.035590887069702,
      "learning_rate": 6.389413988657846e-05,
      "loss": 0.6938,
      "step": 197
    },
    {
      "epoch": 0.37096018735362996,
      "grad_norm": 1.29371976852417,
      "learning_rate": 6.370510396975425e-05,
      "loss": 0.564,
      "step": 198
    },
    {
      "epoch": 0.3728337236533958,
      "grad_norm": 1.6114490032196045,
      "learning_rate": 6.351606805293005e-05,
      "loss": 0.6462,
      "step": 199
    },
    {
      "epoch": 0.3747072599531616,
      "grad_norm": 1.3183594942092896,
      "learning_rate": 6.332703213610587e-05,
      "loss": 0.703,
      "step": 200
    },
    {
      "epoch": 0.3765807962529274,
      "grad_norm": 1.2881242036819458,
      "learning_rate": 6.313799621928167e-05,
      "loss": 0.5733,
      "step": 201
    },
    {
      "epoch": 0.37845433255269323,
      "grad_norm": 1.2699567079544067,
      "learning_rate": 6.294896030245747e-05,
      "loss": 0.4214,
      "step": 202
    },
    {
      "epoch": 0.380327868852459,
      "grad_norm": 1.5764131546020508,
      "learning_rate": 6.275992438563328e-05,
      "loss": 0.6954,
      "step": 203
    },
    {
      "epoch": 0.38220140515222484,
      "grad_norm": 1.7201733589172363,
      "learning_rate": 6.257088846880907e-05,
      "loss": 0.5927,
      "step": 204
    },
    {
      "epoch": 0.3840749414519906,
      "grad_norm": 1.6457407474517822,
      "learning_rate": 6.238185255198487e-05,
      "loss": 0.884,
      "step": 205
    },
    {
      "epoch": 0.38594847775175645,
      "grad_norm": 2.185868740081787,
      "learning_rate": 6.219281663516069e-05,
      "loss": 0.6095,
      "step": 206
    },
    {
      "epoch": 0.3878220140515222,
      "grad_norm": 1.1203902959823608,
      "learning_rate": 6.200378071833649e-05,
      "loss": 0.4456,
      "step": 207
    },
    {
      "epoch": 0.38969555035128806,
      "grad_norm": 1.0084385871887207,
      "learning_rate": 6.18147448015123e-05,
      "loss": 0.55,
      "step": 208
    },
    {
      "epoch": 0.3915690866510539,
      "grad_norm": 1.4076883792877197,
      "learning_rate": 6.16257088846881e-05,
      "loss": 0.5107,
      "step": 209
    },
    {
      "epoch": 0.39344262295081966,
      "grad_norm": 2.394566297531128,
      "learning_rate": 6.143667296786389e-05,
      "loss": 0.3598,
      "step": 210
    },
    {
      "epoch": 0.3953161592505855,
      "grad_norm": 1.7736097574234009,
      "learning_rate": 6.124763705103969e-05,
      "loss": 0.5296,
      "step": 211
    },
    {
      "epoch": 0.3971896955503513,
      "grad_norm": 2.0989441871643066,
      "learning_rate": 6.10586011342155e-05,
      "loss": 0.6425,
      "step": 212
    },
    {
      "epoch": 0.3990632318501171,
      "grad_norm": 1.225581407546997,
      "learning_rate": 6.086956521739131e-05,
      "loss": 0.4771,
      "step": 213
    },
    {
      "epoch": 0.4009367681498829,
      "grad_norm": 1.3986561298370361,
      "learning_rate": 6.0680529300567116e-05,
      "loss": 0.4329,
      "step": 214
    },
    {
      "epoch": 0.4028103044496487,
      "grad_norm": 1.4873288869857788,
      "learning_rate": 6.049149338374291e-05,
      "loss": 0.5313,
      "step": 215
    },
    {
      "epoch": 0.40468384074941455,
      "grad_norm": 1.4026060104370117,
      "learning_rate": 6.0302457466918716e-05,
      "loss": 0.4964,
      "step": 216
    },
    {
      "epoch": 0.4065573770491803,
      "grad_norm": 1.549972653388977,
      "learning_rate": 6.011342155009452e-05,
      "loss": 0.5832,
      "step": 217
    },
    {
      "epoch": 0.40843091334894616,
      "grad_norm": 1.1396701335906982,
      "learning_rate": 5.9924385633270316e-05,
      "loss": 0.497,
      "step": 218
    },
    {
      "epoch": 0.41030444964871193,
      "grad_norm": 1.9451794624328613,
      "learning_rate": 5.973534971644613e-05,
      "loss": 0.4716,
      "step": 219
    },
    {
      "epoch": 0.41217798594847777,
      "grad_norm": 1.2467126846313477,
      "learning_rate": 5.9546313799621937e-05,
      "loss": 0.4469,
      "step": 220
    },
    {
      "epoch": 0.41405152224824354,
      "grad_norm": 3.0501811504364014,
      "learning_rate": 5.935727788279773e-05,
      "loss": 0.7781,
      "step": 221
    },
    {
      "epoch": 0.4159250585480094,
      "grad_norm": 1.3498470783233643,
      "learning_rate": 5.9168241965973537e-05,
      "loss": 0.4556,
      "step": 222
    },
    {
      "epoch": 0.41779859484777515,
      "grad_norm": 1.3163347244262695,
      "learning_rate": 5.897920604914934e-05,
      "loss": 0.4036,
      "step": 223
    },
    {
      "epoch": 0.419672131147541,
      "grad_norm": 1.661955714225769,
      "learning_rate": 5.879017013232514e-05,
      "loss": 0.5179,
      "step": 224
    },
    {
      "epoch": 0.4215456674473068,
      "grad_norm": 1.8297172784805298,
      "learning_rate": 5.8601134215500954e-05,
      "loss": 0.6732,
      "step": 225
    },
    {
      "epoch": 0.4234192037470726,
      "grad_norm": 1.1082801818847656,
      "learning_rate": 5.841209829867676e-05,
      "loss": 0.3367,
      "step": 226
    },
    {
      "epoch": 0.4252927400468384,
      "grad_norm": 1.9292253255844116,
      "learning_rate": 5.8223062381852554e-05,
      "loss": 0.5787,
      "step": 227
    },
    {
      "epoch": 0.4271662763466042,
      "grad_norm": 2.3653767108917236,
      "learning_rate": 5.803402646502836e-05,
      "loss": 0.9874,
      "step": 228
    },
    {
      "epoch": 0.42903981264637003,
      "grad_norm": 2.118072509765625,
      "learning_rate": 5.784499054820416e-05,
      "loss": 0.7436,
      "step": 229
    },
    {
      "epoch": 0.4309133489461358,
      "grad_norm": 2.233647346496582,
      "learning_rate": 5.765595463137996e-05,
      "loss": 0.5792,
      "step": 230
    },
    {
      "epoch": 0.43278688524590164,
      "grad_norm": 0.8989977240562439,
      "learning_rate": 5.7466918714555774e-05,
      "loss": 0.5655,
      "step": 231
    },
    {
      "epoch": 0.4346604215456675,
      "grad_norm": 1.4450442790985107,
      "learning_rate": 5.727788279773158e-05,
      "loss": 0.5496,
      "step": 232
    },
    {
      "epoch": 0.43653395784543325,
      "grad_norm": 1.544945478439331,
      "learning_rate": 5.7088846880907374e-05,
      "loss": 0.4687,
      "step": 233
    },
    {
      "epoch": 0.4384074941451991,
      "grad_norm": 1.6816496849060059,
      "learning_rate": 5.689981096408318e-05,
      "loss": 0.5819,
      "step": 234
    },
    {
      "epoch": 0.44028103044496486,
      "grad_norm": 1.858669400215149,
      "learning_rate": 5.671077504725898e-05,
      "loss": 0.6063,
      "step": 235
    },
    {
      "epoch": 0.4421545667447307,
      "grad_norm": 1.8927433490753174,
      "learning_rate": 5.652173913043478e-05,
      "loss": 0.4498,
      "step": 236
    },
    {
      "epoch": 0.44402810304449647,
      "grad_norm": 1.3629258871078491,
      "learning_rate": 5.6332703213610595e-05,
      "loss": 0.4746,
      "step": 237
    },
    {
      "epoch": 0.4459016393442623,
      "grad_norm": 1.5777112245559692,
      "learning_rate": 5.61436672967864e-05,
      "loss": 0.4977,
      "step": 238
    },
    {
      "epoch": 0.4477751756440281,
      "grad_norm": 1.3294203281402588,
      "learning_rate": 5.5954631379962195e-05,
      "loss": 0.5266,
      "step": 239
    },
    {
      "epoch": 0.4496487119437939,
      "grad_norm": 1.8685741424560547,
      "learning_rate": 5.5765595463138e-05,
      "loss": 0.6728,
      "step": 240
    },
    {
      "epoch": 0.45152224824355974,
      "grad_norm": 1.9407883882522583,
      "learning_rate": 5.55765595463138e-05,
      "loss": 0.5681,
      "step": 241
    },
    {
      "epoch": 0.4533957845433255,
      "grad_norm": 1.8125547170639038,
      "learning_rate": 5.5387523629489605e-05,
      "loss": 0.4493,
      "step": 242
    },
    {
      "epoch": 0.45526932084309135,
      "grad_norm": 1.625563621520996,
      "learning_rate": 5.5198487712665415e-05,
      "loss": 0.587,
      "step": 243
    },
    {
      "epoch": 0.45714285714285713,
      "grad_norm": 2.0424907207489014,
      "learning_rate": 5.500945179584122e-05,
      "loss": 0.4457,
      "step": 244
    },
    {
      "epoch": 0.45901639344262296,
      "grad_norm": 2.762009859085083,
      "learning_rate": 5.4820415879017015e-05,
      "loss": 0.7478,
      "step": 245
    },
    {
      "epoch": 0.46088992974238874,
      "grad_norm": 1.2234716415405273,
      "learning_rate": 5.463137996219282e-05,
      "loss": 0.6805,
      "step": 246
    },
    {
      "epoch": 0.46276346604215457,
      "grad_norm": 1.9273598194122314,
      "learning_rate": 5.444234404536862e-05,
      "loss": 0.6316,
      "step": 247
    },
    {
      "epoch": 0.4646370023419204,
      "grad_norm": 1.0851058959960938,
      "learning_rate": 5.4253308128544425e-05,
      "loss": 0.4108,
      "step": 248
    },
    {
      "epoch": 0.4665105386416862,
      "grad_norm": 1.4053884744644165,
      "learning_rate": 5.4064272211720236e-05,
      "loss": 0.5735,
      "step": 249
    },
    {
      "epoch": 0.468384074941452,
      "grad_norm": 1.460370421409607,
      "learning_rate": 5.387523629489604e-05,
      "loss": 0.4244,
      "step": 250
    },
    {
      "epoch": 0.4702576112412178,
      "grad_norm": 1.3282382488250732,
      "learning_rate": 5.3686200378071836e-05,
      "loss": 0.3272,
      "step": 251
    },
    {
      "epoch": 0.4721311475409836,
      "grad_norm": 2.0525882244110107,
      "learning_rate": 5.349716446124764e-05,
      "loss": 0.6256,
      "step": 252
    },
    {
      "epoch": 0.4740046838407494,
      "grad_norm": 1.3066959381103516,
      "learning_rate": 5.330812854442344e-05,
      "loss": 0.5619,
      "step": 253
    },
    {
      "epoch": 0.47587822014051523,
      "grad_norm": 1.4472954273223877,
      "learning_rate": 5.3119092627599246e-05,
      "loss": 0.429,
      "step": 254
    },
    {
      "epoch": 0.477751756440281,
      "grad_norm": 2.0796926021575928,
      "learning_rate": 5.293005671077504e-05,
      "loss": 0.557,
      "step": 255
    },
    {
      "epoch": 0.47962529274004684,
      "grad_norm": 1.1626571416854858,
      "learning_rate": 5.274102079395086e-05,
      "loss": 0.3301,
      "step": 256
    },
    {
      "epoch": 0.48149882903981267,
      "grad_norm": 1.5845972299575806,
      "learning_rate": 5.2551984877126656e-05,
      "loss": 0.7689,
      "step": 257
    },
    {
      "epoch": 0.48337236533957845,
      "grad_norm": 1.3259246349334717,
      "learning_rate": 5.236294896030246e-05,
      "loss": 0.4985,
      "step": 258
    },
    {
      "epoch": 0.4852459016393443,
      "grad_norm": 1.7995549440383911,
      "learning_rate": 5.217391304347826e-05,
      "loss": 0.6027,
      "step": 259
    },
    {
      "epoch": 0.48711943793911006,
      "grad_norm": 1.4890034198760986,
      "learning_rate": 5.1984877126654066e-05,
      "loss": 0.574,
      "step": 260
    },
    {
      "epoch": 0.4889929742388759,
      "grad_norm": 1.9065916538238525,
      "learning_rate": 5.179584120982986e-05,
      "loss": 0.6625,
      "step": 261
    },
    {
      "epoch": 0.49086651053864166,
      "grad_norm": 1.3144745826721191,
      "learning_rate": 5.160680529300568e-05,
      "loss": 0.568,
      "step": 262
    },
    {
      "epoch": 0.4927400468384075,
      "grad_norm": 1.256780982017517,
      "learning_rate": 5.141776937618148e-05,
      "loss": 0.4076,
      "step": 263
    },
    {
      "epoch": 0.49461358313817333,
      "grad_norm": 1.3335691690444946,
      "learning_rate": 5.122873345935728e-05,
      "loss": 0.5257,
      "step": 264
    },
    {
      "epoch": 0.4964871194379391,
      "grad_norm": 2.5340020656585693,
      "learning_rate": 5.1039697542533084e-05,
      "loss": 0.7713,
      "step": 265
    },
    {
      "epoch": 0.49836065573770494,
      "grad_norm": 1.3049139976501465,
      "learning_rate": 5.085066162570889e-05,
      "loss": 0.4832,
      "step": 266
    },
    {
      "epoch": 0.5002341920374708,
      "grad_norm": 1.3628064393997192,
      "learning_rate": 5.0661625708884684e-05,
      "loss": 0.481,
      "step": 267
    },
    {
      "epoch": 0.5021077283372365,
      "grad_norm": 2.1640379428863525,
      "learning_rate": 5.04725897920605e-05,
      "loss": 0.6764,
      "step": 268
    },
    {
      "epoch": 0.5039812646370023,
      "grad_norm": 2.1556081771850586,
      "learning_rate": 5.02835538752363e-05,
      "loss": 0.9038,
      "step": 269
    },
    {
      "epoch": 0.5058548009367682,
      "grad_norm": 2.632594347000122,
      "learning_rate": 5.00945179584121e-05,
      "loss": 0.6245,
      "step": 270
    },
    {
      "epoch": 0.507728337236534,
      "grad_norm": 1.5461677312850952,
      "learning_rate": 4.9905482041587904e-05,
      "loss": 0.5269,
      "step": 271
    },
    {
      "epoch": 0.5096018735362998,
      "grad_norm": 1.5268797874450684,
      "learning_rate": 4.971644612476371e-05,
      "loss": 0.462,
      "step": 272
    },
    {
      "epoch": 0.5114754098360655,
      "grad_norm": 1.9091745615005493,
      "learning_rate": 4.952741020793951e-05,
      "loss": 0.5125,
      "step": 273
    },
    {
      "epoch": 0.5133489461358314,
      "grad_norm": 2.4205620288848877,
      "learning_rate": 4.9338374291115314e-05,
      "loss": 0.5357,
      "step": 274
    },
    {
      "epoch": 0.5152224824355972,
      "grad_norm": 1.8165756464004517,
      "learning_rate": 4.914933837429112e-05,
      "loss": 0.6167,
      "step": 275
    },
    {
      "epoch": 0.517096018735363,
      "grad_norm": 1.648453950881958,
      "learning_rate": 4.896030245746692e-05,
      "loss": 0.5255,
      "step": 276
    },
    {
      "epoch": 0.5189695550351288,
      "grad_norm": 1.5760940313339233,
      "learning_rate": 4.8771266540642725e-05,
      "loss": 0.553,
      "step": 277
    },
    {
      "epoch": 0.5208430913348946,
      "grad_norm": 1.2111490964889526,
      "learning_rate": 4.858223062381853e-05,
      "loss": 0.422,
      "step": 278
    },
    {
      "epoch": 0.5227166276346604,
      "grad_norm": 2.9945576190948486,
      "learning_rate": 4.839319470699433e-05,
      "loss": 0.6027,
      "step": 279
    },
    {
      "epoch": 0.5245901639344263,
      "grad_norm": 1.3284084796905518,
      "learning_rate": 4.8204158790170135e-05,
      "loss": 0.5085,
      "step": 280
    },
    {
      "epoch": 0.5264637002341921,
      "grad_norm": 1.5404034852981567,
      "learning_rate": 4.801512287334594e-05,
      "loss": 0.5715,
      "step": 281
    },
    {
      "epoch": 0.5283372365339578,
      "grad_norm": 1.4205551147460938,
      "learning_rate": 4.782608695652174e-05,
      "loss": 0.5111,
      "step": 282
    },
    {
      "epoch": 0.5302107728337236,
      "grad_norm": 2.1471610069274902,
      "learning_rate": 4.7637051039697545e-05,
      "loss": 0.5963,
      "step": 283
    },
    {
      "epoch": 0.5320843091334895,
      "grad_norm": 1.8367687463760376,
      "learning_rate": 4.744801512287335e-05,
      "loss": 0.4429,
      "step": 284
    },
    {
      "epoch": 0.5339578454332553,
      "grad_norm": 1.7746849060058594,
      "learning_rate": 4.725897920604915e-05,
      "loss": 0.4779,
      "step": 285
    },
    {
      "epoch": 0.535831381733021,
      "grad_norm": 1.8827158212661743,
      "learning_rate": 4.7069943289224955e-05,
      "loss": 0.7345,
      "step": 286
    },
    {
      "epoch": 0.5377049180327869,
      "grad_norm": 1.6051805019378662,
      "learning_rate": 4.688090737240076e-05,
      "loss": 0.4752,
      "step": 287
    },
    {
      "epoch": 0.5395784543325527,
      "grad_norm": 1.5654784440994263,
      "learning_rate": 4.669187145557656e-05,
      "loss": 0.5077,
      "step": 288
    },
    {
      "epoch": 0.5414519906323185,
      "grad_norm": 1.3724365234375,
      "learning_rate": 4.6502835538752366e-05,
      "loss": 0.3983,
      "step": 289
    },
    {
      "epoch": 0.5433255269320844,
      "grad_norm": 2.4171907901763916,
      "learning_rate": 4.631379962192817e-05,
      "loss": 0.7719,
      "step": 290
    },
    {
      "epoch": 0.5451990632318501,
      "grad_norm": 1.5241870880126953,
      "learning_rate": 4.612476370510397e-05,
      "loss": 0.5705,
      "step": 291
    },
    {
      "epoch": 0.5470725995316159,
      "grad_norm": 1.2201149463653564,
      "learning_rate": 4.5935727788279776e-05,
      "loss": 0.5006,
      "step": 292
    },
    {
      "epoch": 0.5489461358313817,
      "grad_norm": 2.381991386413574,
      "learning_rate": 4.574669187145558e-05,
      "loss": 0.5739,
      "step": 293
    },
    {
      "epoch": 0.5508196721311476,
      "grad_norm": 1.6965610980987549,
      "learning_rate": 4.555765595463138e-05,
      "loss": 0.4716,
      "step": 294
    },
    {
      "epoch": 0.5526932084309133,
      "grad_norm": 1.5849553346633911,
      "learning_rate": 4.5368620037807186e-05,
      "loss": 0.578,
      "step": 295
    },
    {
      "epoch": 0.5545667447306791,
      "grad_norm": 2.1918601989746094,
      "learning_rate": 4.517958412098299e-05,
      "loss": 0.7424,
      "step": 296
    },
    {
      "epoch": 0.556440281030445,
      "grad_norm": 1.9013601541519165,
      "learning_rate": 4.499054820415879e-05,
      "loss": 0.5918,
      "step": 297
    },
    {
      "epoch": 0.5583138173302108,
      "grad_norm": 1.583003044128418,
      "learning_rate": 4.4801512287334596e-05,
      "loss": 0.6906,
      "step": 298
    },
    {
      "epoch": 0.5601873536299766,
      "grad_norm": 1.2257803678512573,
      "learning_rate": 4.46124763705104e-05,
      "loss": 0.4684,
      "step": 299
    },
    {
      "epoch": 0.5620608899297423,
      "grad_norm": 1.55916166305542,
      "learning_rate": 4.44234404536862e-05,
      "loss": 0.5819,
      "step": 300
    },
    {
      "epoch": 0.5639344262295082,
      "grad_norm": 1.6532882452011108,
      "learning_rate": 4.423440453686201e-05,
      "loss": 0.5167,
      "step": 301
    },
    {
      "epoch": 0.565807962529274,
      "grad_norm": 1.5301122665405273,
      "learning_rate": 4.404536862003781e-05,
      "loss": 0.6219,
      "step": 302
    },
    {
      "epoch": 0.5676814988290398,
      "grad_norm": 1.838187336921692,
      "learning_rate": 4.3856332703213614e-05,
      "loss": 0.6103,
      "step": 303
    },
    {
      "epoch": 0.5695550351288057,
      "grad_norm": 1.7164677381515503,
      "learning_rate": 4.366729678638942e-05,
      "loss": 0.5579,
      "step": 304
    },
    {
      "epoch": 0.5714285714285714,
      "grad_norm": 1.4162155389785767,
      "learning_rate": 4.347826086956522e-05,
      "loss": 0.4095,
      "step": 305
    },
    {
      "epoch": 0.5733021077283372,
      "grad_norm": 1.2484432458877563,
      "learning_rate": 4.3289224952741024e-05,
      "loss": 0.4253,
      "step": 306
    },
    {
      "epoch": 0.5751756440281031,
      "grad_norm": 2.022376298904419,
      "learning_rate": 4.310018903591683e-05,
      "loss": 0.5083,
      "step": 307
    },
    {
      "epoch": 0.5770491803278689,
      "grad_norm": 1.1720447540283203,
      "learning_rate": 4.291115311909263e-05,
      "loss": 0.3439,
      "step": 308
    },
    {
      "epoch": 0.5789227166276346,
      "grad_norm": 1.1296247243881226,
      "learning_rate": 4.2722117202268434e-05,
      "loss": 0.3821,
      "step": 309
    },
    {
      "epoch": 0.5807962529274004,
      "grad_norm": 1.4668363332748413,
      "learning_rate": 4.253308128544424e-05,
      "loss": 0.4468,
      "step": 310
    },
    {
      "epoch": 0.5826697892271663,
      "grad_norm": 1.0149601697921753,
      "learning_rate": 4.234404536862004e-05,
      "loss": 0.3897,
      "step": 311
    },
    {
      "epoch": 0.5845433255269321,
      "grad_norm": 1.143051266670227,
      "learning_rate": 4.215500945179584e-05,
      "loss": 0.5595,
      "step": 312
    },
    {
      "epoch": 0.5864168618266979,
      "grad_norm": 1.1801575422286987,
      "learning_rate": 4.196597353497165e-05,
      "loss": 0.4987,
      "step": 313
    },
    {
      "epoch": 0.5882903981264637,
      "grad_norm": 1.5171469449996948,
      "learning_rate": 4.177693761814745e-05,
      "loss": 0.5449,
      "step": 314
    },
    {
      "epoch": 0.5901639344262295,
      "grad_norm": 2.1527211666107178,
      "learning_rate": 4.158790170132325e-05,
      "loss": 0.5503,
      "step": 315
    },
    {
      "epoch": 0.5920374707259953,
      "grad_norm": 2.414015531539917,
      "learning_rate": 4.139886578449906e-05,
      "loss": 0.6269,
      "step": 316
    },
    {
      "epoch": 0.5939110070257612,
      "grad_norm": 1.5757980346679688,
      "learning_rate": 4.120982986767486e-05,
      "loss": 0.5281,
      "step": 317
    },
    {
      "epoch": 0.5957845433255269,
      "grad_norm": 2.1231565475463867,
      "learning_rate": 4.102079395085066e-05,
      "loss": 0.4302,
      "step": 318
    },
    {
      "epoch": 0.5976580796252927,
      "grad_norm": 2.842323064804077,
      "learning_rate": 4.083175803402647e-05,
      "loss": 0.6301,
      "step": 319
    },
    {
      "epoch": 0.5995316159250585,
      "grad_norm": 1.6331439018249512,
      "learning_rate": 4.064272211720227e-05,
      "loss": 0.5946,
      "step": 320
    },
    {
      "epoch": 0.6014051522248244,
      "grad_norm": 1.5859971046447754,
      "learning_rate": 4.045368620037807e-05,
      "loss": 0.5575,
      "step": 321
    },
    {
      "epoch": 0.6032786885245902,
      "grad_norm": 1.9403761625289917,
      "learning_rate": 4.026465028355388e-05,
      "loss": 0.5931,
      "step": 322
    },
    {
      "epoch": 0.6051522248243559,
      "grad_norm": 2.1092324256896973,
      "learning_rate": 4.007561436672968e-05,
      "loss": 0.8223,
      "step": 323
    },
    {
      "epoch": 0.6070257611241218,
      "grad_norm": 1.2987226247787476,
      "learning_rate": 3.988657844990548e-05,
      "loss": 0.4634,
      "step": 324
    },
    {
      "epoch": 0.6088992974238876,
      "grad_norm": 2.923494815826416,
      "learning_rate": 3.969754253308129e-05,
      "loss": 0.705,
      "step": 325
    },
    {
      "epoch": 0.6107728337236534,
      "grad_norm": 1.4888219833374023,
      "learning_rate": 3.950850661625709e-05,
      "loss": 0.6184,
      "step": 326
    },
    {
      "epoch": 0.6126463700234192,
      "grad_norm": 2.1686503887176514,
      "learning_rate": 3.931947069943289e-05,
      "loss": 0.4601,
      "step": 327
    },
    {
      "epoch": 0.614519906323185,
      "grad_norm": 1.3330702781677246,
      "learning_rate": 3.91304347826087e-05,
      "loss": 0.4675,
      "step": 328
    },
    {
      "epoch": 0.6163934426229508,
      "grad_norm": 1.2939376831054688,
      "learning_rate": 3.89413988657845e-05,
      "loss": 0.4577,
      "step": 329
    },
    {
      "epoch": 0.6182669789227166,
      "grad_norm": 1.1211572885513306,
      "learning_rate": 3.87523629489603e-05,
      "loss": 0.2398,
      "step": 330
    },
    {
      "epoch": 0.6201405152224825,
      "grad_norm": 1.396148920059204,
      "learning_rate": 3.856332703213611e-05,
      "loss": 0.5252,
      "step": 331
    },
    {
      "epoch": 0.6220140515222482,
      "grad_norm": 2.1459386348724365,
      "learning_rate": 3.837429111531191e-05,
      "loss": 0.7995,
      "step": 332
    },
    {
      "epoch": 0.623887587822014,
      "grad_norm": 2.2155373096466064,
      "learning_rate": 3.8185255198487716e-05,
      "loss": 0.596,
      "step": 333
    },
    {
      "epoch": 0.6257611241217799,
      "grad_norm": 1.6520990133285522,
      "learning_rate": 3.799621928166352e-05,
      "loss": 0.5026,
      "step": 334
    },
    {
      "epoch": 0.6276346604215457,
      "grad_norm": 1.4496448040008545,
      "learning_rate": 3.780718336483932e-05,
      "loss": 0.5273,
      "step": 335
    },
    {
      "epoch": 0.6295081967213115,
      "grad_norm": 2.3701658248901367,
      "learning_rate": 3.7618147448015126e-05,
      "loss": 0.5936,
      "step": 336
    },
    {
      "epoch": 0.6313817330210773,
      "grad_norm": 1.0773242712020874,
      "learning_rate": 3.742911153119093e-05,
      "loss": 0.5463,
      "step": 337
    },
    {
      "epoch": 0.6332552693208431,
      "grad_norm": 1.111262321472168,
      "learning_rate": 3.724007561436673e-05,
      "loss": 0.4663,
      "step": 338
    },
    {
      "epoch": 0.6351288056206089,
      "grad_norm": 1.5521594285964966,
      "learning_rate": 3.7051039697542537e-05,
      "loss": 0.4183,
      "step": 339
    },
    {
      "epoch": 0.6370023419203747,
      "grad_norm": 1.8290311098098755,
      "learning_rate": 3.686200378071834e-05,
      "loss": 0.4309,
      "step": 340
    },
    {
      "epoch": 0.6388758782201405,
      "grad_norm": 1.7908934354782104,
      "learning_rate": 3.6672967863894143e-05,
      "loss": 0.4161,
      "step": 341
    },
    {
      "epoch": 0.6407494145199063,
      "grad_norm": 1.2204185724258423,
      "learning_rate": 3.648393194706995e-05,
      "loss": 0.3449,
      "step": 342
    },
    {
      "epoch": 0.6426229508196721,
      "grad_norm": 1.3382617235183716,
      "learning_rate": 3.629489603024575e-05,
      "loss": 0.4232,
      "step": 343
    },
    {
      "epoch": 0.644496487119438,
      "grad_norm": 1.6252886056900024,
      "learning_rate": 3.6105860113421554e-05,
      "loss": 0.4048,
      "step": 344
    },
    {
      "epoch": 0.6463700234192038,
      "grad_norm": 2.9409546852111816,
      "learning_rate": 3.591682419659736e-05,
      "loss": 0.5707,
      "step": 345
    },
    {
      "epoch": 0.6482435597189695,
      "grad_norm": 2.6037418842315674,
      "learning_rate": 3.572778827977316e-05,
      "loss": 0.5734,
      "step": 346
    },
    {
      "epoch": 0.6501170960187354,
      "grad_norm": 2.0565853118896484,
      "learning_rate": 3.5538752362948964e-05,
      "loss": 0.3638,
      "step": 347
    },
    {
      "epoch": 0.6519906323185012,
      "grad_norm": 2.297574520111084,
      "learning_rate": 3.534971644612477e-05,
      "loss": 0.3456,
      "step": 348
    },
    {
      "epoch": 0.653864168618267,
      "grad_norm": 1.6796213388442993,
      "learning_rate": 3.516068052930057e-05,
      "loss": 0.5908,
      "step": 349
    },
    {
      "epoch": 0.6557377049180327,
      "grad_norm": 1.855683445930481,
      "learning_rate": 3.4971644612476374e-05,
      "loss": 0.5003,
      "step": 350
    },
    {
      "epoch": 0.6576112412177986,
      "grad_norm": 1.633160948753357,
      "learning_rate": 3.478260869565218e-05,
      "loss": 0.3422,
      "step": 351
    },
    {
      "epoch": 0.6594847775175644,
      "grad_norm": 1.6053845882415771,
      "learning_rate": 3.4593572778827974e-05,
      "loss": 0.4036,
      "step": 352
    },
    {
      "epoch": 0.6613583138173302,
      "grad_norm": 1.684324026107788,
      "learning_rate": 3.4404536862003784e-05,
      "loss": 0.3078,
      "step": 353
    },
    {
      "epoch": 0.6632318501170961,
      "grad_norm": 1.4779924154281616,
      "learning_rate": 3.421550094517959e-05,
      "loss": 0.3505,
      "step": 354
    },
    {
      "epoch": 0.6651053864168618,
      "grad_norm": 1.9489833116531372,
      "learning_rate": 3.4026465028355385e-05,
      "loss": 0.5597,
      "step": 355
    },
    {
      "epoch": 0.6669789227166276,
      "grad_norm": 1.8194040060043335,
      "learning_rate": 3.3837429111531195e-05,
      "loss": 0.4692,
      "step": 356
    },
    {
      "epoch": 0.6688524590163935,
      "grad_norm": 1.812927007675171,
      "learning_rate": 3.3648393194707e-05,
      "loss": 0.5607,
      "step": 357
    },
    {
      "epoch": 0.6707259953161593,
      "grad_norm": 1.4554117918014526,
      "learning_rate": 3.3459357277882795e-05,
      "loss": 0.6294,
      "step": 358
    },
    {
      "epoch": 0.672599531615925,
      "grad_norm": 1.1532193422317505,
      "learning_rate": 3.3270321361058605e-05,
      "loss": 0.5405,
      "step": 359
    },
    {
      "epoch": 0.6744730679156908,
      "grad_norm": 1.5922812223434448,
      "learning_rate": 3.308128544423441e-05,
      "loss": 0.3837,
      "step": 360
    },
    {
      "epoch": 0.6763466042154567,
      "grad_norm": 1.8116129636764526,
      "learning_rate": 3.2892249527410205e-05,
      "loss": 0.6025,
      "step": 361
    },
    {
      "epoch": 0.6782201405152225,
      "grad_norm": 2.08189058303833,
      "learning_rate": 3.2703213610586015e-05,
      "loss": 0.7466,
      "step": 362
    },
    {
      "epoch": 0.6800936768149883,
      "grad_norm": 2.170393228530884,
      "learning_rate": 3.251417769376182e-05,
      "loss": 0.4761,
      "step": 363
    },
    {
      "epoch": 0.6819672131147541,
      "grad_norm": 2.5162055492401123,
      "learning_rate": 3.2325141776937615e-05,
      "loss": 0.6545,
      "step": 364
    },
    {
      "epoch": 0.6838407494145199,
      "grad_norm": 1.4463143348693848,
      "learning_rate": 3.2136105860113426e-05,
      "loss": 0.4191,
      "step": 365
    },
    {
      "epoch": 0.6857142857142857,
      "grad_norm": 1.562599539756775,
      "learning_rate": 3.194706994328923e-05,
      "loss": 0.5107,
      "step": 366
    },
    {
      "epoch": 0.6875878220140516,
      "grad_norm": 1.5575300455093384,
      "learning_rate": 3.1758034026465026e-05,
      "loss": 0.5717,
      "step": 367
    },
    {
      "epoch": 0.6894613583138174,
      "grad_norm": 0.8908594250679016,
      "learning_rate": 3.1568998109640836e-05,
      "loss": 0.3293,
      "step": 368
    },
    {
      "epoch": 0.6913348946135831,
      "grad_norm": 2.1447675228118896,
      "learning_rate": 3.137996219281664e-05,
      "loss": 0.6551,
      "step": 369
    },
    {
      "epoch": 0.6932084309133489,
      "grad_norm": 2.7910733222961426,
      "learning_rate": 3.1190926275992436e-05,
      "loss": 0.5097,
      "step": 370
    },
    {
      "epoch": 0.6950819672131148,
      "grad_norm": 1.3425980806350708,
      "learning_rate": 3.1001890359168246e-05,
      "loss": 0.3718,
      "step": 371
    },
    {
      "epoch": 0.6969555035128806,
      "grad_norm": 2.5459039211273193,
      "learning_rate": 3.081285444234405e-05,
      "loss": 0.4936,
      "step": 372
    },
    {
      "epoch": 0.6988290398126463,
      "grad_norm": 1.188934564590454,
      "learning_rate": 3.0623818525519846e-05,
      "loss": 0.3952,
      "step": 373
    },
    {
      "epoch": 0.7007025761124122,
      "grad_norm": 1.3077971935272217,
      "learning_rate": 3.0434782608695656e-05,
      "loss": 0.48,
      "step": 374
    },
    {
      "epoch": 0.702576112412178,
      "grad_norm": 2.0081934928894043,
      "learning_rate": 3.0245746691871456e-05,
      "loss": 0.4495,
      "step": 375
    },
    {
      "epoch": 0.7044496487119438,
      "grad_norm": 2.64565110206604,
      "learning_rate": 3.005671077504726e-05,
      "loss": 0.6622,
      "step": 376
    },
    {
      "epoch": 0.7063231850117097,
      "grad_norm": 2.3017916679382324,
      "learning_rate": 2.9867674858223067e-05,
      "loss": 0.4707,
      "step": 377
    },
    {
      "epoch": 0.7081967213114754,
      "grad_norm": 1.2907477617263794,
      "learning_rate": 2.9678638941398867e-05,
      "loss": 0.3465,
      "step": 378
    },
    {
      "epoch": 0.7100702576112412,
      "grad_norm": 1.4940789937973022,
      "learning_rate": 2.948960302457467e-05,
      "loss": 0.4295,
      "step": 379
    },
    {
      "epoch": 0.711943793911007,
      "grad_norm": 2.3190572261810303,
      "learning_rate": 2.9300567107750477e-05,
      "loss": 0.4813,
      "step": 380
    },
    {
      "epoch": 0.7138173302107729,
      "grad_norm": 1.5624690055847168,
      "learning_rate": 2.9111531190926277e-05,
      "loss": 0.3952,
      "step": 381
    },
    {
      "epoch": 0.7156908665105386,
      "grad_norm": 1.4924473762512207,
      "learning_rate": 2.892249527410208e-05,
      "loss": 0.5674,
      "step": 382
    },
    {
      "epoch": 0.7175644028103044,
      "grad_norm": 2.227761745452881,
      "learning_rate": 2.8733459357277887e-05,
      "loss": 0.6166,
      "step": 383
    },
    {
      "epoch": 0.7194379391100703,
      "grad_norm": 3.056320905685425,
      "learning_rate": 2.8544423440453687e-05,
      "loss": 0.7896,
      "step": 384
    },
    {
      "epoch": 0.7213114754098361,
      "grad_norm": 3.502265453338623,
      "learning_rate": 2.835538752362949e-05,
      "loss": 0.7545,
      "step": 385
    },
    {
      "epoch": 0.7231850117096019,
      "grad_norm": 1.3643419742584229,
      "learning_rate": 2.8166351606805297e-05,
      "loss": 0.4302,
      "step": 386
    },
    {
      "epoch": 0.7250585480093676,
      "grad_norm": 2.206794500350952,
      "learning_rate": 2.7977315689981097e-05,
      "loss": 0.4658,
      "step": 387
    },
    {
      "epoch": 0.7269320843091335,
      "grad_norm": 1.960006594657898,
      "learning_rate": 2.77882797731569e-05,
      "loss": 0.5017,
      "step": 388
    },
    {
      "epoch": 0.7288056206088993,
      "grad_norm": 1.6084755659103394,
      "learning_rate": 2.7599243856332708e-05,
      "loss": 0.5362,
      "step": 389
    },
    {
      "epoch": 0.7306791569086651,
      "grad_norm": 2.202829122543335,
      "learning_rate": 2.7410207939508508e-05,
      "loss": 0.6057,
      "step": 390
    },
    {
      "epoch": 0.7325526932084309,
      "grad_norm": 1.1395479440689087,
      "learning_rate": 2.722117202268431e-05,
      "loss": 0.4508,
      "step": 391
    },
    {
      "epoch": 0.7344262295081967,
      "grad_norm": 2.21488356590271,
      "learning_rate": 2.7032136105860118e-05,
      "loss": 0.519,
      "step": 392
    },
    {
      "epoch": 0.7362997658079625,
      "grad_norm": 1.3417634963989258,
      "learning_rate": 2.6843100189035918e-05,
      "loss": 0.4205,
      "step": 393
    },
    {
      "epoch": 0.7381733021077284,
      "grad_norm": 1.19857919216156,
      "learning_rate": 2.665406427221172e-05,
      "loss": 0.3989,
      "step": 394
    },
    {
      "epoch": 0.7400468384074942,
      "grad_norm": 1.588720440864563,
      "learning_rate": 2.646502835538752e-05,
      "loss": 0.4319,
      "step": 395
    },
    {
      "epoch": 0.7419203747072599,
      "grad_norm": 1.4736754894256592,
      "learning_rate": 2.6275992438563328e-05,
      "loss": 0.5389,
      "step": 396
    },
    {
      "epoch": 0.7437939110070257,
      "grad_norm": 1.3349685668945312,
      "learning_rate": 2.608695652173913e-05,
      "loss": 0.4275,
      "step": 397
    },
    {
      "epoch": 0.7456674473067916,
      "grad_norm": 1.0540331602096558,
      "learning_rate": 2.589792060491493e-05,
      "loss": 0.3136,
      "step": 398
    },
    {
      "epoch": 0.7475409836065574,
      "grad_norm": 2.090968132019043,
      "learning_rate": 2.570888468809074e-05,
      "loss": 0.4152,
      "step": 399
    },
    {
      "epoch": 0.7494145199063232,
      "grad_norm": 0.9632964730262756,
      "learning_rate": 2.5519848771266542e-05,
      "loss": 0.3634,
      "step": 400
    }
  ],
  "logging_steps": 1,
  "max_steps": 534,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 200,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.442740583049339e+17,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
